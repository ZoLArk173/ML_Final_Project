{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install numpy pandas\n",
    "!pip3 install scikit-learn\n",
    "!pip3 install feature_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.linear_model import LogisticRegression, HuberRegressor\n",
    "from feature_engine.encoding import WoEEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"./tabular-playground-series-aug-2022\"\n",
    "seed = 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is mainly from https://www.kaggle.com/code/nourhadrich/tps-aug-neural-network\n",
    "def preprocessing(df_train, df_test):\n",
    "    data = pd.concat([df_train, df_test])\n",
    "\n",
    "    data['m3_missing'] = data['measurement_3'].isnull().astype(np.int8)\n",
    "    data['m5_missing'] = data['measurement_5'].isnull().astype(np.int8)\n",
    "    data['area'] = data['attribute_2'] * data['attribute_3']\n",
    "\n",
    "    feature = [f for f in df_test.columns if f.startswith(\n",
    "        'measurement') or f == 'loading']\n",
    "\n",
    "    # dictionnary of dictionnaries (for the 11 best correlated measurement columns),\n",
    "    # we will use the dictionnaries below to select the best correlated columns according to the product code)\n",
    "    # Only for 'measurement_17' we make a 'manual' selection :\n",
    "    full_fill_dict = {}\n",
    "    full_fill_dict['measurement_17'] = {\n",
    "        'A': ['measurement_5', 'measurement_6', 'measurement_8'],\n",
    "        'B': ['measurement_4', 'measurement_5', 'measurement_7'],\n",
    "        'C': ['measurement_5', 'measurement_7', 'measurement_8', 'measurement_9'],\n",
    "        'D': ['measurement_5', 'measurement_6', 'measurement_7', 'measurement_8'],\n",
    "        'E': ['measurement_4', 'measurement_5', 'measurement_6', 'measurement_8'],\n",
    "        'F': ['measurement_4', 'measurement_5', 'measurement_6', 'measurement_7'],\n",
    "        'G': ['measurement_4', 'measurement_6', 'measurement_8', 'measurement_9'],\n",
    "        'H': ['measurement_4', 'measurement_5', 'measurement_7', 'measurement_8', 'measurement_9'],\n",
    "        'I': ['measurement_3', 'measurement_7', 'measurement_8']\n",
    "    }\n",
    "\n",
    "    # collect the name of the next 10 best measurement columns sorted by correlation (except 17 already done above):\n",
    "    col = [col for col in df_test.columns if 'measurement' not in col] + \\\n",
    "        ['loading', 'm3_missing', 'm5_missing']\n",
    "    a = []\n",
    "    b = []\n",
    "    for x in range(3, 17):\n",
    "        corr = np.absolute(data.drop(col, axis=1).corr()[\n",
    "                           f'measurement_{x}']).sort_values(ascending=False)\n",
    "        # we add the 3 first lines of the correlation values to get the \"most correlated\"\n",
    "        a.append(np.round(np.sum(corr[1:4]), 3))\n",
    "        b.append(f'measurement_{x}')\n",
    "    c = pd.DataFrame()\n",
    "    c['Selected columns'] = b\n",
    "    c['correlation total'] = a\n",
    "    c = c.sort_values(by='correlation total',\n",
    "                      ascending=False).reset_index(drop=True)\n",
    "\n",
    "    for i in range(10):\n",
    "        # we select the next best correlated column\n",
    "        measurement_col = 'measurement_' + c.iloc[i, 0][12:]\n",
    "        fill_dict = {}\n",
    "        for x in data.product_code.unique():\n",
    "            corr = np.absolute(data[data.product_code == x].drop(col, axis=1).corr()[\n",
    "                               measurement_col]).sort_values(ascending=False)\n",
    "            measurement_col_dic = {}\n",
    "            measurement_col_dic[measurement_col] = corr[1:5].index.tolist()\n",
    "            fill_dict[x] = measurement_col_dic[measurement_col]\n",
    "        full_fill_dict[measurement_col] = fill_dict\n",
    "\n",
    "    feature = [f for f in data.columns if f.startswith(\n",
    "        'measurement') or f == 'loading']\n",
    "\n",
    "    nullValue_cols = [\n",
    "        col for col in df_train.columns if df_train[col].isnull().sum() != 0]\n",
    "\n",
    "    for code in data.product_code.unique():\n",
    "        for measurement_col in list(full_fill_dict.keys()):\n",
    "            tmp = data[data.product_code == code]\n",
    "            column = full_fill_dict[measurement_col][code]\n",
    "            tmp_train = tmp[column+[measurement_col]].dropna(how='any')\n",
    "            tmp_test = tmp[(tmp[column].isnull().sum(axis=1) == 0)\n",
    "                           & (tmp[measurement_col].isnull())]\n",
    "\n",
    "            model = HuberRegressor(epsilon=1.9)\n",
    "            model.fit(tmp_train[column], tmp_train[measurement_col])\n",
    "            data.loc[(data.product_code == code) & (data[column].isnull().sum(axis=1) == 0) & (\n",
    "                data[measurement_col].isnull()), measurement_col] = model.predict(tmp_test[column])\n",
    "\n",
    "        # others NA columns:\n",
    "        NA = data.loc[data[\"product_code\"] == code,\n",
    "                      nullValue_cols].isnull().sum().sum()\n",
    "        model1 = KNNImputer(n_neighbors=3)\n",
    "        data.loc[data.product_code == code, feature] = model1.fit_transform(\n",
    "            data.loc[data.product_code == code, feature])\n",
    "\n",
    "    data['measurement_avg'] = data[[\n",
    "        f'measurement_{i}' for i in range(3, 17)]].mean(axis=1)\n",
    "    data['measurement_std'] = data[[\n",
    "        f'measurement_{i}' for i in range(3, 17)]].std(axis=1)\n",
    "    data['measurement_median'] = data[[\n",
    "        f'measurement_{i}' for i in range(3, 17)]].median(axis=1)\n",
    "    data['measurement_max'] = data[[\n",
    "        f'measurement_{i}' for i in range(3, 17)]].max(axis=1)\n",
    "    data['measurement_min'] = data[[\n",
    "        f'measurement_{i}' for i in range(3, 17)]].min(axis=1)\n",
    "    data['measurement_skew'] = data[[\n",
    "        f'measurement_{i}' for i in range(3, 17)]].skew(axis=1)\n",
    "\n",
    "    df_train = data.iloc[:df_train.shape[0], :]\n",
    "    df_test = data.iloc[df_train.shape[0]:, :]\n",
    "\n",
    "    woe_encoder = WoEEncoder(variables=['attribute_0'])\n",
    "    woe_encoder.fit(df_train, df_train['failure'])\n",
    "    df_train = woe_encoder.transform(df_train)\n",
    "    df_test = woe_encoder.transform(df_test)\n",
    "\n",
    "    selector = SelectKBest(k=5)\n",
    "    selector.fit(df_train.iloc[:, 5:25], df_train['failure'])\n",
    "    features = selector.get_support(indices=True)\n",
    "    features = [df_train.columns[x+5] for x in features]\n",
    "    features = np.append(features, ['loading', 'attribute_0', 'area',\n",
    "                                    'm3_missing', 'm5_missing', 'measurement_avg',\n",
    "                                    'measurement_std', 'measurement_median', 'measurement_min',\n",
    "                                    'measurement_skew'])\n",
    "    print(features)\n",
    "\n",
    "    # features = ['loading', 'attribute_0',\n",
    "    #             'measurement_17', 'measurement_0',\n",
    "    #             'measurement_1', 'measurement_2', 'area',\n",
    "    #             'm3_missing', 'm5_missing', 'measurement_avg',\n",
    "    #             'measurement_std', 'measurement_median', 'measurement_min',\n",
    "    #             'measurement_skew']\n",
    "\n",
    "    return df_train, df_test, features\n",
    "\n",
    "\n",
    "df_train, df_test, features = preprocessing(pd.read_csv(\n",
    "    f'{DATA_PATH}/train.csv'), pd.read_csv(f'{DATA_PATH}/test.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPreprocessing(filePath: str, submit: bool):\n",
    "    trainData = pd.read_csv(filePath)\n",
    "\n",
    "    if submit:\n",
    "        y = trainData.failure.values\n",
    "    else:\n",
    "        id = trainData.loc[:, 'id'].values\n",
    "    x = trainData.loc[:, 'product_code':'measurement_17'].values\n",
    "\n",
    "    # average = {}\n",
    "\n",
    "    # for row in x:\n",
    "    #     if not average.get(row[0]):\n",
    "    #         average[row[0]] = [[] for i in range(len(row) - 1)]\n",
    "\n",
    "    #     for val, averageArr in zip(row[1:], average[row[0]]):\n",
    "    #         if type(val) != str and not math.isnan(val):\n",
    "    #             averageArr.append(val)\n",
    "\n",
    "    # for key in average:\n",
    "    #     newArr = []\n",
    "    #     for averageArr in average[key]:\n",
    "    #         newArr.append(np.mean(averageArr))\n",
    "    #     average[key] = newArr\n",
    "\n",
    "    codes = {}\n",
    "    xx = [0 for i in range(len(x))]  # Storing new table\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        # for j in range(len(x[i][1:])):\n",
    "        #     if type(x[i][j+1]) != str and math.isnan(x[i][j+1]):\n",
    "        #         x[i][j+1] = average[x[i][0]][j]\n",
    "        x[i][0] = ord(x[i][0]) - ord('A')\n",
    "\n",
    "        if not codes.get(x[i][0]):\n",
    "            codes[x[i][0]] = 0\n",
    "        codes[x[i][0]] += 1\n",
    "\n",
    "        x[i][2] = int(x[i][2][-1])\n",
    "        x[i][3] = int(x[i][3][-1])\n",
    "        xx[i] = [\n",
    "            1 if math.isnan(x[i][9]) else 0,  # measurement 3 missing\n",
    "            1 if math.isnan(x[i][11]) else 0  # measurement 5 missing\n",
    "        ]\n",
    "\n",
    "    print(codes)\n",
    "\n",
    "    imputer = KNNImputer(n_neighbors=3)\n",
    "    idx = 0\n",
    "    for code, num in codes.items():\n",
    "        x[idx:idx+num+1] = imputer.fit_transform(x[idx:idx+num+1])\n",
    "        idx += num\n",
    "\n",
    "    # Index(['id', 'product_code', 'loading', 'attribute_0', 'attribute_1',\n",
    "#        'attribute_2', 'attribute_3', 'measurement_0', 'measurement_1',\n",
    "#        'measurement_2', 'measurement_3', 'measurement_4', 'measurement_5',\n",
    "#        'measurement_6', 'measurement_7', 'measurement_8', 'measurement_9',\n",
    "#        'measurement_10', 'measurement_11', 'measurement_12', 'measurement_13',\n",
    "#        'measurement_14', 'measurement_15', 'measurement_16', 'measurement_17',\n",
    "#        'failure'],\n",
    "    for i in range(len(x)):\n",
    "        xx[i] = np.append(x[i][1], xx[i])\n",
    "        xx[i] = np.append(x[i][2], xx[i])\n",
    "        xx[i] = np.append(x[i][6:], xx[i])\n",
    "        xx[i] = np.append(x[i][4] * x[i][5], xx[i])\n",
    "\n",
    "    x = np.array(xx).astype(float)\n",
    "    if submit:\n",
    "        y = y.astype(float)\n",
    "\n",
    "    if submit:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            x, y, test_size=0.2, random_state=69)\n",
    "    else:\n",
    "        x_train, x_test, y_train, y_test = id, x, None, None\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    if submit:\n",
    "        scaler.fit(x_train)\n",
    "    else:\n",
    "        scaler.fit(x_test)\n",
    "\n",
    "    if submit:\n",
    "        x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "# x_train, x_valid, y_train, y_valid = dataPreprocessing(f'{DATA_PATH}/train.csv', True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(\n",
    "    df_train[features].values, df_train['failure'].values, test_size=0.1, random_state=seed)\n",
    "\n",
    "\n",
    "# NN = MLPRegressor(hidden_layer_sizes=(64, 32),\n",
    "#                   random_state=seed, verbose=True, early_stopping=True)\n",
    "# NN.fit(x_train, y_train)\n",
    "\n",
    "# SVM = svm.NuSVR(verbose=True)\n",
    "# SVM.fit(x_train, y_train)\n",
    "\n",
    "pred = []\n",
    "score = []\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(df_train[features].values)):\n",
    "    LR = LogisticRegression(max_iter=1000, penalty='l2',\n",
    "                            solver='newton-cg', random_state=seed)\n",
    "    x_data, y_data = df_train[features].values, df_train['failure'].values\n",
    "    x_train, x_valid = x_data[train_index], x_data[test_index]\n",
    "    y_train, y_valid = y_data[train_index], y_data[test_index]\n",
    "    LR.fit(x_train, y_train)\n",
    "\n",
    "    # outputs = NN.predict(x_valid)\n",
    "    # outputs =  SVM.predict(x_valid)\n",
    "    outputs = LR.predict_proba(x_valid)[:, 1]\n",
    "\n",
    "    print(outputs)\n",
    "\n",
    "    # for i in range(len(outputs)):\n",
    "    #     if outputs[i] < 0:\n",
    "    #         outputs[i] = 0.0\n",
    "    #     if outputs[i] > 1:\n",
    "    #         outputs[i] = 1.0\n",
    "\n",
    "    print(roc_auc_score(y_valid, outputs))\n",
    "\n",
    "    pred.append(LR.predict_proba(df_test[features].values)[:, 1])\n",
    "    score.append((roc_auc_score(y_valid, outputs), len(pred)-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ids, x_submit, _, _ = dataPreprocessing(f'{DATA_PATH}/test.csv', False)\n",
    "\n",
    "# outputs = NN.predict(x_submit)\n",
    "\n",
    "# test_pred = NN.predict(df_test[features].values)\n",
    "\n",
    "score.sort(reverse=True)\n",
    "print(score)\n",
    "weight = [0.4, 0.3, 0.3]\n",
    "test_pred = pred[score[0][1]] * weight[0] + \\\n",
    "    pred[score[1][1]] * weight[1] + \\\n",
    "    pred[score[2][1]] * weight[2]\n",
    "\n",
    "# for i in range(len(test_pred)):\n",
    "#     if test_pred[i] < 0:\n",
    "#         test_pred[i] = 0.0\n",
    "#     if test_pred[i] > 1:\n",
    "#         test_pred[i] = 1.0\n",
    "print(test_pred[0:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train does not output prediction csv\n",
    "# csvFile = open('submission.csv', 'w', newline='')\n",
    "# csv_writer = csv.writer(csvFile)\n",
    "# csv_writer.writerow([\"id\", \"failure\"])\n",
    "\n",
    "# for id, output in zip(df_test['id'].values, test_pred):\n",
    "#     # for id, output in zip(ids, outputs):\n",
    "#     csv_writer.writerow([id, output])\n",
    "\n",
    "# csvFile.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88050ca507c59d18391192168513c082718f7f36049c91282137273e4cf0ed3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
